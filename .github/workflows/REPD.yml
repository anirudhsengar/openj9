name: GlitchWitcher - Bug Prediction Analysis

on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: write
  issues: write
  contents: read

jobs:
  glitch-witcher:
    runs-on: ubuntu-latest
    if: contains(github.event.comment.body, 'GlitchWitcher')

    steps:
      - name: Parse GlitchWitcher Command
        id: parse-command
        run: |
          comment_body="${{ github.event.comment.body }}"
          echo "Full comment: $comment_body"

          # Extract PR link if provided
          pr_link=""
          if echo "$comment_body" | grep -oE 'GlitchWitcher\s+https://github\.com/[^/]+/[^/]+/pull/[0-9]+'; then
            pr_link=$(echo "$comment_body" | grep -oE 'https://github\.com/[^/]+/[^/]+/pull/[0-9]+')
            echo "PR link provided: $pr_link"
          elif echo "$comment_body" | grep -oE 'GlitchWitcher\s*$'; then
            # No PR link provided, use current PR if comment is on a PR
            if [ "${{ github.event.issue.pull_request.url }}" != "" ]; then
              pr_link="${{ github.event.issue.pull_request.html_url }}"
              echo "Using current PR: $pr_link"
            else
              echo "ERROR: GlitchWitcher called on issue without PR link"
              exit 1
            fi
          else
            echo "ERROR: Invalid GlitchWitcher command format"
            exit 1
          fi

          # Extract repository info from PR link
          if [[ "$pr_link" =~ https://github\.com/([^/]+)/([^/]+)/pull/([0-9]+) ]]; then
            repo_owner="${BASH_REMATCH[1]}"
            repo_name="${BASH_REMATCH[2]}"
            pr_number="${BASH_REMATCH[3]}"
            full_repo_name="${repo_owner}-${repo_name}"
            
            echo "repo_owner=$repo_owner" >> $GITHUB_OUTPUT
            echo "repo_name=$repo_name" >> $GITHUB_OUTPUT
            echo "pr_number=$pr_number" >> $GITHUB_OUTPUT
            echo "full_repo_name=$full_repo_name" >> $GITHUB_OUTPUT
            echo "pr_link=$pr_link" >> $GITHUB_OUTPUT
            echo "repo_url=https://github.com/$repo_owner/$repo_name.git" >> $GITHUB_OUTPUT
          else
            echo "ERROR: Could not parse repository info from PR link: $pr_link"
            exit 1
          fi

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install tensorflow==2.12.0 pandas joblib scipy numpy urllib3 scikit-learn
          sudo apt-get update
          sudo apt-get install -y cloc git

      - name: Download scripts
        run: |
          echo "Downloading scripts from aqa-test-tools repository..."
          curl -L -o ExtractTraditionalFeatures.sh "https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/ExtractTraditionalFeatures.sh"
          curl -L -o extract_traditional_features.sh "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/extract_traditional_features.sh"
          curl -L -o save_trained_model.py "https://raw.githubusercontent.com/anirudhsengar/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/save_trained_model.py"
          curl -L -o REPD_Impl.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/REPD_Impl.py"
          curl -L -o autoencoder.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/autoencoder.py"
          curl -L -o stat_util.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/stat_util.py"
          chmod +x ExtractTraditionalFeatures.sh
          chmod +x extract_traditional_features.sh

          # Create predict.py with the exact same functions as the old working version
          cat > predict.py << 'EOF'
          import pandas as pd
          from REPD_Impl import REPD
          from autoencoder import AutoEncoder
          import warnings
          import tensorflow.compat.v1 as tf
          import numpy as np
          import json
          import sys
          import os
          import scipy.stats as st

          # Suppress warnings
          tf.disable_v2_behavior()
          warnings.simplefilter("ignore")

          def format_predictions(predictions):
              """Format PDF predictions for display"""
              results = []
              
              print(f"Debug: Predictions shape: {predictions.shape}", file=sys.stderr)
              print(f"Debug: Predictions content: {predictions}", file=sys.stderr)
              
              # Now predictions should be (n_samples, 2) - no more 3D arrays!
              for i in range(predictions.shape[0]):
                  pred = predictions[i]
                  print(f"Debug: Processing prediction {i}: {pred}", file=sys.stderr)
                  
                  # Extract probabilities
                  if isinstance(pred, np.ndarray) and len(pred) >= 2:
                      p_defective = float(pred[0])
                      p_non_defective = float(pred[1])
                  else:
                      print(f"Warning: Unexpected prediction format for {i}: {pred}", file=sys.stderr)
                      p_defective = 0.0
                      p_non_defective = 0.0
                  
                  results.append({
                      'p_defective': p_defective,
                      'p_non_defective': p_non_defective
                  })
                  
                  print(f"Debug: File {i} - P(Defective): {p_defective}, P(Non-Defective): {p_non_defective}", file=sys.stderr)
              
              return results

          def format_results_for_comparison(file_names, base_data, head_data):
              """Format results as tables comparing BEFORE and AFTER for each file"""
              
              # First, calculate percentage changes for all files to determine sorting order
              file_changes = []
              
              for i, file_name in enumerate(file_names):
                  if i < len(base_data) and i < len(head_data):
                      base_defective = base_data[i]['p_defective']
                      base_non_defective = base_data[i]['p_non_defective']
                      head_defective = head_data[i]['p_defective']
                      head_non_defective = head_data[i]['p_non_defective']
                      
                      # Calculate percentage changes
                      if base_defective != 0:
                          defective_change = ((head_defective - base_defective) / abs(base_defective)) * 100
                      else:
                          defective_change = 0 if head_defective == 0 else float('inf')
                      
                      if base_non_defective != 0:
                          non_defective_change = ((head_non_defective - base_non_defective) / abs(base_non_defective)) * 100
                      else:
                          non_defective_change = 0 if head_non_defective == 0 else float('inf')
                      
                      # Use the maximum absolute change for sorting
                      max_change = max(abs(defective_change), abs(non_defective_change))
                      
                      file_changes.append({
                          'index': i,
                          'file_name': file_name,
                          'max_change': max_change,
                          'defective_change': defective_change,
                          'non_defective_change': non_defective_change,
                          'base_defective': base_defective,
                          'base_non_defective': base_non_defective,
                          'head_defective': head_defective,
                          'head_non_defective': head_non_defective
                      })
                  else:
                      file_changes.append({
                          'index': i,
                          'file_name': file_name,
                          'max_change': 0,
                          'error': True
                      })
              
              # Sort files by maximum percentage change in descending order
              file_changes.sort(key=lambda x: x['max_change'], reverse=True)
              
              # Generate output with sorted files
              output = ["## ðŸ“Š Bug Prediction Analysis\n"]
              
              for file_data in file_changes:
                  file_name = file_data['file_name']
                  output.append(f"#### File: `{file_name}`\n")
                  
                  if 'error' in file_data:
                      output.append("| Status |")
                      output.append("|--------|")
                      output.append("| Error: Prediction data not available |")
                      output.append("")
                  else:
                      base_defective = file_data['base_defective']
                      base_non_defective = file_data['base_non_defective']
                      head_defective = file_data['head_defective']
                      head_non_defective = file_data['head_non_defective']
                      defective_change = file_data['defective_change']
                      non_defective_change = file_data['non_defective_change']
                      
                      # Format percentage change values
                      def format_change(change_val):
                          if change_val == float('inf'):
                              return "âˆž%"
                          elif change_val == float('-inf'):
                              return "-âˆž%"
                          else:
                              return f"{change_val:+.2f}%"

                      before = "Defective" if base_defective > base_non_defective else "Non-Defective"
                      after = "Defective" if head_defective > head_non_defective else "Non-Defective"

                      output.append("Outcome: " + before + " -> " + after)
                      
                      # Create table with 4 columns
                      output.append("| Metric | BEFORE PR | AFTER PR | % Change |")
                      output.append("|--------|-----------|----------|----------|")
                      output.append(f"| PDF(Defective \\| Reconstruction Error) | {base_defective} | {head_defective} | {format_change(defective_change)} |")
                      output.append(f"| PDF(Non-Defective \\| Reconstruction Error) | {base_non_defective} | {head_non_defective} | {format_change(non_defective_change)} |")
                      output.append("")
              
              return "\n".join(output)

          def format_results(file_names, prediction_data):
              """Format results with probability values (for individual calls)"""
              results = []
              
              for i, file_name in enumerate(file_names):
                  if i < len(prediction_data):
                      p_defective = prediction_data[i]['p_defective']
                      p_non_defective = prediction_data[i]['p_non_defective']
                      
                      results.append({
                          'file': file_name,
                          'p_defective': p_defective,
                          'p_non_defective': p_non_defective
                      })
                  else:
                      results.append({
                          'file': file_name,
                          'error': 'No prediction available'
                      })
              
              return results

          def get_distribution_class(dist_name):
              """Get the distribution class (not frozen) from scipy.stats"""
              if dist_name is None:
                  return None
              
              try:
                  dist_class = getattr(st, dist_name)
                  return dist_class
              except Exception as e:
                  return None

          def load_trained_model(model_dir="trained_model"):
              """Load the pre-trained model"""
              if not os.path.exists(model_dir):
                  raise FileNotFoundError(f"Trained model not found at {model_dir}. Please ensure the model is trained and saved.")
              
              # Load metadata from JSON
              metadata_path = os.path.join(model_dir, "metadata.json")
              if not os.path.exists(metadata_path):
                  raise FileNotFoundError(f"Model metadata not found at {metadata_path}")
              
              with open(metadata_path, 'r') as f:
                  metadata = json.load(f)
              
              # Load REPD classifier parameters from JSON
              classifier_params_path = os.path.join(model_dir, "classifier_params.json")
              if not os.path.exists(classifier_params_path):
                  raise FileNotFoundError(f"Classifier parameters not found at {classifier_params_path}")
              
              with open(classifier_params_path, 'r') as f:
                  classifier_params = json.load(f)
                  
              # Recreate the autoencoder with saved architecture
              autoencoder = AutoEncoder(
                  metadata['architecture'], 
                  metadata['learning_rate'], 
                  metadata['epochs'], 
                  metadata['batch_size']
              )
              
              # Load the saved autoencoder weights
              autoencoder_path = os.path.join(model_dir, "autoencoder")
              autoencoder.load(autoencoder_path)
              
              # Recreate REPD classifier
              classifier = REPD(autoencoder)
                  
              # Non-defective distribution
              classifier.dnd = get_distribution_class(classifier_params.get('dnd_name'))
              classifier.dnd_pa = tuple(classifier_params.get('dnd_params', []))
              
              # Defective distribution  
              classifier.dd = get_distribution_class(classifier_params.get('dd_name'))
              classifier.dd_pa = tuple(classifier_params.get('dd_params', []))
              
              # Check if distributions were created successfully
              if classifier.dnd is None:
                  raise ValueError("Failed to get non-defective distribution class")
              if classifier.dd is None:
                  raise ValueError("Failed to get defective distribution class")
              
              return classifier

          def predict(features_file, model_dir="trained_model"):
              """Make predictions using pre-trained model"""
              
              classifier = load_trained_model(model_dir)
              
              # Load test data
              df_test = pd.read_csv(features_file)
              
              # Check if CSV has data rows (more than just header)
              if len(df_test) == 0:
                  print("No files to analyze.")
                  return []
              
              file_names = df_test["File"].values
              X_test = df_test.drop(columns=["File"]).values
              
              print(f"Debug: Processing {len(file_names)} files", file=sys.stderr)
              print(f"Debug: File names: {file_names}", file=sys.stderr)
              print(f"Debug: X_test shape: {X_test.shape}", file=sys.stderr)
                          
              # Make predictions (PDF values)
              pdf_predictions = classifier.predict(X_test)
              print(f"Debug: Predictions shape: {pdf_predictions.shape}", file=sys.stderr)
              print(f"Debug: Predictions type: {type(pdf_predictions)}", file=sys.stderr)
              
              # Format predictions for display
              prediction_data = format_predictions(pdf_predictions)

              # Format and return results
              results = format_results(file_names, prediction_data)
              
              # Close the session
              classifier.dim_reduction_model.close()
              
              return results

          if __name__ == "__main__":
              if len(sys.argv) != 2:
                  print("Usage: python predict.py <path_to_features.csv>")
                  print("Make sure the trained model exists in the 'trained_model' directory.")
                  sys.exit(1)
              
              features_csv_path = sys.argv[1]
              results = predict(features_csv_path)
              
              # For command line usage, print individual results
              for result in results:
                  if 'error' in result:
                      print(f"File: {result['file']}")
                      print(f"Error: {result['error']}")
                  else:
                      print(f"File: {result['file']}")
                      print(f"PDF(Defective | Reconstruction Error): {result['p_defective']}")
                      print(f"PDF(Non-Defective | Reconstruction Error): {result['p_non_defective']}")
                  print()
          EOF

      - name: Check Dataset Availability
        id: check-dataset
        run: |
          dataset_url="https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/${{ steps.parse-command.outputs.full_repo_name }}/${{ steps.parse-command.outputs.full_repo_name }}.csv"
          base_model_url="https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/${{ steps.parse-command.outputs.full_repo_name }}/trained_model"
          echo "Checking dataset availability..."
          echo "Dataset URL: $dataset_url"
          echo "Model base URL: $base_model_url"

          dataset_exists="false"
          model_exists="false"

          # Check if dataset CSV exists
          if curl -sI --fail "$dataset_url" > /dev/null 2>&1; then
            echo "Dataset CSV found for ${{ steps.parse-command.outputs.full_repo_name }}"
            dataset_exists="true"
          else
            echo "Dataset CSV not found for ${{ steps.parse-command.outputs.full_repo_name }}"
          fi

          # Check if TF model artifacts exist (no model.pkl in this repo)
          if \
            curl -sI --fail "$base_model_url/metadata.json" > /dev/null 2>&1 && \
            curl -sI --fail "$base_model_url/classifier_params.json" > /dev/null 2>&1 && \
            curl -sI --fail "$base_model_url/autoencoder.index" > /dev/null 2>&1 && \
            curl -sI --fail "$base_model_url/autoencoder.meta" > /dev/null 2>&1 && \
            curl -sI --fail "$base_model_url/autoencoder.data-00000-of-00001" > /dev/null 2>&1 && \
            curl -sI --fail "$base_model_url/checkpoint" > /dev/null 2>&1
          then
            echo "TensorFlow model artifacts found"
            model_exists="true"
          else
            echo "TensorFlow model artifacts not found or incomplete"
          fi

          echo "dataset_exists=$dataset_exists" >> $GITHUB_OUTPUT
          echo "model_exists=$model_exists" >> $GITHUB_OUTPUT
          echo "dataset_url=$dataset_url" >> $GITHUB_OUTPUT
          echo "base_model_url=$base_model_url" >> $GITHUB_OUTPUT

      - name: Generate Dataset if Missing
        if: steps.check-dataset.outputs.dataset_exists == 'false'
        run: |
          echo "Generating dataset for ${{ steps.parse-command.outputs.full_repo_name }}..."
          ./ExtractTraditionalFeatures.sh "${{ steps.parse-command.outputs.repo_url }}"

          # Find the generated CSV file
          csv_file=$(find . -name "*.csv" -path "./metrics_output_*" | head -1)
          if [ ! -f "$csv_file" ]; then
            echo "ERROR: Failed to generate CSV file"
            exit 1
          fi

          echo "Generated CSV file: $csv_file"
          echo "csv_file_path=$csv_file" >> $GITHUB_ENV

      - name: Train Model if Missing
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        run: |
          echo "Training model for ${{ steps.parse-command.outputs.full_repo_name }}..."

          # Determine dataset path
          if [ "${{ steps.check-dataset.outputs.dataset_exists }}" == "true" ]; then
            dataset_path="${{ steps.check-dataset.outputs.dataset_url }}"
          else
            dataset_path="$csv_file_path"
          fi

          echo "Using dataset: $dataset_path"
          python3 save_trained_model.py "$dataset_path"

          if [ ! -d "trained_model" ]; then
            echo "ERROR: Failed to generate trained model"
            exit 1
          fi

          echo "Model training completed successfully"

      - name: Create PR to aqa-triage-data
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        run: |
          echo "Creating PR to anirudhsengar/aqa-triage-data..."

          # Configure git user
          git config --global user.name "GlitchWitcher Bot"
          git config --global user.email "glitchwicher-bot@adoptium.net"

          # Clone the aqa-triage-data repository using PAT in URL
          git clone https://${{ secrets.TRIAGE_PAT }}@github.com/anirudhsengar/aqa-triage-data.git
          cd aqa-triage-data

          # Create target directory
          target_dir="GlitchWitcher/Traditional Dataset/${{ steps.parse-command.outputs.full_repo_name }}"
          mkdir -p "$target_dir"

          # Copy files to target directory
          if [ "${{ steps.check-dataset.outputs.dataset_exists }}" == "false" ]; then
            echo "Copying CSV file..."
            cp "../$csv_file_path" "$target_dir/${{ steps.parse-command.outputs.full_repo_name }}.csv"
          fi

          if [ "${{ steps.check-dataset.outputs.model_exists }}" == "false" ]; then
            echo "Copying trained model..."
            cp -r "../trained_model" "$target_dir/"
          fi

          # Create branch and commit
          branch_name="add-dataset-${{ steps.parse-command.outputs.full_repo_name }}-$(date +%Y%m%d-%H%M%S)"
          git checkout -b "$branch_name"
          git add .
          git commit -m "Add dataset and trained model for ${{ steps.parse-command.outputs.full_repo_name }}"

          # Push branch using PAT in URL
          git push https://${{ secrets.TRIAGE_PAT }}@github.com/anirudhsengar/aqa-triage-data.git "$branch_name"

          # Create PR using GitHub API
          cat > pr_body.md << EOF
          # GlitchWitcher Dataset Addition

          This PR adds the dataset and trained model for repository: **${{ steps.parse-command.outputs.repo_owner }}/${{ steps.parse-command.outputs.repo_name }}**

          ## Contents:
          - Dataset CSV file: \`${{ steps.parse-command.outputs.full_repo_name }}.csv\`
          - Trained model directory: \`trained_model/\`

          ## Triggered by:
          - Comment in: ${{ github.event.issue.html_url }}
          - Target PR: ${{ steps.parse-command.outputs.pr_link }}

          This PR was automatically generated by the GlitchWitcher workflow.
          EOF

          # Create PR with GitHub API
          echo "Creating PR..."
          PR_RESPONSE=$(curl -X POST \
            -H "Authorization: token ${{ secrets.TRIAGE_PAT }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/anirudhsengar/aqa-triage-data/pulls \
            -d '{
              "title": "Add GlitchWitcher dataset for ${{ steps.parse-command.outputs.full_repo_name }}",
              "head": "'$branch_name'",
              "base": "main",
              "body": "'"$(cat pr_body.md | sed 's/"/\\"/g' | tr '\n' ' ')"'",
              "draft": false
            }')

          echo "API Response received, parsing..."
          
          # Extract PR number and URL using improved parsing logic
          if command -v python3 &> /dev/null; then
            PR_NUMBER=$(echo "$PR_RESPONSE" | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('number', ''))")
            PR_URL=$(echo "$PR_RESPONSE" | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('html_url', ''))")
          else
            # Fallback to grep/sed parsing with improved regex
            PR_NUMBER=$(echo "$PR_RESPONSE" | grep -o '"number"[ ]*:[ ]*[0-9][0-9]*' | head -1 | sed 's/.*:[ ]*//')
            PR_URL=$(echo "$PR_RESPONSE" | grep -o '"html_url"[ ]*:[ ]*"[^"]*"' | head -1 | sed 's/.*:[ ]*"\([^"]*\)".*/\1/')
          fi
          
          echo "Extracted PR_NUMBER: '$PR_NUMBER'"
          echo "Extracted PR_URL: '$PR_URL'"
          
          if [ ! -z "$PR_NUMBER" ] && [ "$PR_NUMBER" != "null" ] && [ ! -z "$PR_URL" ] && [ "$PR_URL" != "null" ]; then
            echo "âœ… Successfully created PR #$PR_NUMBER"
            echo "PR URL: $PR_URL"
            
            # Enable auto-merge using the PR number
            echo "Enabling auto-merge for PR #$PR_NUMBER..."
            MERGE_RESPONSE=$(curl -X PUT \
              -H "Authorization: token ${{ secrets.TRIAGE_PAT }}" \
              -H "Accept: application/vnd.github.v3+json" \
              https://api.github.com/repos/anirudhsengar/aqa-triage-data/pulls/$PR_NUMBER/merge \
              -d '{
                "commit_title": "Auto-merge: Add GlitchWitcher dataset for ${{ steps.parse-command.outputs.full_repo_name }}",
                "commit_message": "Automatically merged by GlitchWitcher workflow",
                "merge_method": "squash"
              }')
            
            echo "Auto-merge response: $MERGE_RESPONSE"
            
            # Check if merge was successful
            if echo "$MERGE_RESPONSE" | grep -q '"merged": true'; then
              echo "âœ… PR #$PR_NUMBER successfully auto-merged"
            else
              echo "âš ï¸ Auto-merge may have failed, but PR was created successfully"
            fi
          else
            echo "âŒ Failed to parse PR information from response"
            echo "Full Response: $PR_RESPONSE"
            
            # Check if PR was actually created by looking for expected fields
            if echo "$PR_RESPONSE" | grep -q '"html_url"'; then
              echo "âš ï¸ PR may have been created but parsing failed"
              # Try to extract URL manually
              MANUAL_URL=$(echo "$PR_RESPONSE" | grep -o 'https://github.com/[^"]*' | head -1)
              if [ ! -z "$MANUAL_URL" ]; then
                echo "Found PR URL manually: $MANUAL_URL"
                # Try to extract PR number from URL
                MANUAL_PR_NUMBER=$(echo "$MANUAL_URL" | grep -o '/pull/[0-9]*' | sed 's/\/pull\///')
                if [ ! -z "$MANUAL_PR_NUMBER" ]; then
                  echo "Attempting auto-merge with manually extracted PR number: $MANUAL_PR_NUMBER"
                  curl -X PUT \
                    -H "Authorization: token ${{ secrets.TRIAGE_PAT }}" \
                    -H "Accept: application/vnd.github.v3+json" \
                    https://api.github.com/repos/anirudhsengar/aqa-triage-data/pulls/$MANUAL_PR_NUMBER/merge \
                    -d '{
                      "commit_title": "Auto-merge: Add GlitchWitcher dataset for ${{ steps.parse-command.outputs.full_repo_name }}",
                      "commit_message": "Automatically merged by GlitchWitcher workflow",
                      "merge_method": "squash"
                    }'
                fi
              fi
            else
              echo "ERROR: Failed to create PR"
              exit 1
            fi
          fi

      - name: Run Analysis on Target PR
        id: analysis
        run: |
          echo "Running GlitchWitcher analysis on ${{ steps.parse-command.outputs.pr_link }}..."
          # Get PR details using GitHub API
          pr_api_url="https://api.github.com/repos/${{ steps.parse-command.outputs.repo_owner }}/${{ steps.parse-command.outputs.repo_name }}/pulls/${{ steps.parse-command.outputs.pr_number }}"
          pr_info=$(curl -s -H "Accept: application/vnd.github.v3+json" "$pr_api_url")
          base_sha=$(echo "$pr_info" | grep -o '"sha": *"[^"]*"' | head -1 | sed 's/"sha": *"\([^"]*\)"/\1/')
          head_sha=$(echo "$pr_info" | grep -o '"sha": *"[^"]*"' | tail -1 | sed 's/"sha": *"\([^"]*\)"/\1/')

          echo "Base SHA: $base_sha"
          echo "Head SHA: $head_sha"

          # Clone the target repository
          git clone "${{ steps.parse-command.outputs.repo_url }}" target_repo
          cd target_repo

          # Get changed files
          git fetch origin
          changed_files=$(git diff --name-only "$base_sha..$head_sha" | grep -E "\.(c|cpp|cxx|cc|h|hpp|hxx)$" || echo "")

          if [ -z "$changed_files" ]; then
            echo "No C/C++ files changed in this PR"
            echo "analysis_result=No C/C++ files found in the PR changes." >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Changed files: $changed_files"

          # Extract features for base commit
          git checkout "$base_sha"
          mkdir -p ../metrics_output_base
          echo "File,loc,v(g),ev(g),iv(g),n,v,l,d,i,e,b,t,lOComment,lOBlank,LOCodeAndComment,uniq_Op,Uniq_Opnd,total_Op,total_Opnd,branchCount" > ../metrics_output_base/summary_metrics.csv
          for file in $changed_files; do
            if [ -f "$file" ]; then
              echo "Processing $file (base)..."
              ../extract_traditional_features.sh "$file"
              generated_dir=$(ls -td ../metrics_output_* 2>/dev/null | head -n 1)
              if [ -d "$generated_dir" ] && [ -f "$generated_dir/summary_metrics.csv" ]; then
                tail -n +2 "$generated_dir/summary_metrics.csv" >> ../metrics_output_base/summary_metrics.csv
                rm -rf "$generated_dir"
              fi
            fi
          done

          # Extract features for head commit
          git checkout "$head_sha"
          mkdir -p ../metrics_output_head
          echo "File,loc,v(g),ev(g),iv(g),n,v,l,d,i,e,b,t,lOComment,lOBlank,LOCodeAndComment,uniq_Op,Uniq_Opnd,total_Op,total_Opnd,branchCount" > ../metrics_output_head/summary_metrics.csv
          for file in $changed_files; do
            if [ -f "$file" ]; then
              echo "Processing $file (head)..."
              ../extract_traditional_features.sh "$file"
              generated_dir=$(ls -td ../metrics_output_* 2>/dev/null | head -n 1)
              if [ -d "$generated_dir" ] && [ -f "$generated_dir/summary_metrics.csv" ]; then
                tail -n +2 "$generated_dir/summary_metrics.csv" >> ../metrics_output_head/summary_metrics.csv
                rm -rf "$generated_dir"
              fi
            fi
          done

          cd ..

          # Run predictions with TF artifacts
          if [ "${{ steps.check-dataset.outputs.model_exists }}" == "true" ]; then
            echo "Preparing TensorFlow model artifacts..."
            mkdir -p trained_model
            base_url="${{ steps.check-dataset.outputs.base_model_url }}"
            # Fail on missing files to avoid using partial downloads
            curl -fSL -o trained_model/metadata.json            "$base_url/metadata.json"
            curl -fSL -o trained_model/classifier_params.json   "$base_url/classifier_params.json"
            curl -fSL -o trained_model/autoencoder.index        "$base_url/autoencoder.index"
            curl -fSL -o trained_model/autoencoder.meta         "$base_url/autoencoder.meta"
            curl -fSL -o trained_model/autoencoder.data-00000-of-00001 "$base_url/autoencoder.data-00000-of-00001"
            curl -fSL -o trained_model/checkpoint               "$base_url/checkpoint"

            # Create the comparison script exactly like the old working workflow
            cat > compare_predictions.py << 'EOF'
          import sys
          import json
          from predict import predict, format_results_for_comparison

          # Get predictions for base and head
          base_results = predict('metrics_output_base/summary_metrics.csv')
          head_results = predict('metrics_output_head/summary_metrics.csv')

          # Extract file names (should be the same for both)
          file_names = [result['file'] for result in base_results if 'file' in result]

          # Convert to the format expected by format_results_for_comparison
          base_data = [{'p_defective': r['p_defective'], 'p_non_defective': r['p_non_defective']} 
                       for r in base_results if 'p_defective' in r]
          head_data = [{'p_defective': r['p_defective'], 'p_non_defective': r['p_non_defective']} 
                       for r in head_results if 'p_defective' in r]

          # Generate comparison table
          comparison_output = format_results_for_comparison(file_names, base_data, head_data)

          print(comparison_output)
          EOF

            echo "Running comparison predictions..."
            
            # Check if both base and head metrics exist
            if [ -f "./metrics_output_base/summary_metrics.csv" ] && [ -f "./metrics_output_head/summary_metrics.csv" ]; then
              comparison_result=$(python3 compare_predictions.py 2>/dev/null)
              
              if [ $? -eq 0 ] && [ ! -z "$comparison_result" ]; then
                # Use the same output format as the old workflow
                {
                  echo "comment<<EOF"
                  echo "$comparison_result"
                  echo ""
                  echo "### ðŸ“‹ Interpretation Note:"
                  echo "> The values shown are Probability Densities (PDFs), not probabilities. They represent the model's assessment of how likely a file's characteristics are to be 'defective' vs. 'non-defective'. A higher value indicates a better fit for that category. Very small values are expected and normal."
                  echo ""
                  echo "EOF"
                } >> $GITHUB_OUTPUT
              else
                echo "comment=Comparison prediction failed. Model artifacts present but compare_predictions.py invocation failed." >> $GITHUB_OUTPUT
              fi
            else
              echo "comment=Missing base or head metrics files for comparison." >> $GITHUB_OUTPUT
            fi
          else
            echo "No model found for predictions"
            echo "comment=No model found for predictions." >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR
        if: steps.analysis.outputs.comment != ''
        uses: actions/github-script@v6
        env:
          COMMENT_BODY: "${{ steps.analysis.outputs.comment }}"
          PR_LINK: "${{ steps.parse-command.outputs.pr_link }}"
          REPO_NAME: "${{ steps.parse-command.outputs.full_repo_name }}"
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const commentBody = `## ðŸ”® GlitchWitcher Analysis Results

            **Target PR:** ${process.env.PR_LINK}
            **Repository:** ${process.env.REPO_NAME}

            ${process.env.COMMENT_BODY}

            *Analysis performed by GlitchWitcher Bot*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });