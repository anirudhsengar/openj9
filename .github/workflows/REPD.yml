name: GlitchWitcher - Bug Prediction Analysis

on:
  issue_comment:
    types: [created]

permissions:
  pull-requests: write
  issues: write
  contents: read

jobs:
  glitch-witcher:
    runs-on: ubuntu-latest
    if: contains(github.event.comment.body, 'GlitchWitcher')

    steps:
      - name: Parse GlitchWitcher Command
        id: parse-command
        run: |
          comment_body="${{ github.event.comment.body }}"
          echo "Full comment: $comment_body"

          # Extract PR link if provided
          pr_link=""
          if echo "$comment_body" | grep -oE 'GlitchWitcher\s+https://github\.com/[^/]+/[^/]+/pull/[0-9]+'; then
            pr_link=$(echo "$comment_body" | grep -oE 'https://github\.com/[^/]+/[^/]+/pull/[0-9]+')
            echo "PR link provided: $pr_link"
          elif echo "$comment_body" | grep -oE 'GlitchWitcher\s*$'; then
            # No PR link provided, use current PR if comment is on a PR
            if [ "${{ github.event.issue.pull_request.url }}" != "" ]; then
              pr_link="${{ github.event.issue.pull_request.html_url }}"
              echo "Using current PR: $pr_link"
            else
              echo "ERROR: GlitchWitcher called on issue without PR link"
              exit 1
            fi
          else
            echo "ERROR: Invalid GlitchWitcher command format"
            exit 1
          fi

          # Extract repository info from PR link
          if [[ "$pr_link" =~ https://github\.com/([^/]+)/([^/]+)/pull/([0-9]+) ]]; then
            repo_owner="${BASH_REMATCH[1]}"
            repo_name="${BASH_REMATCH[2]}"
            pr_number="${BASH_REMATCH[3]}"
            full_repo_name="${repo_owner}-${repo_name}"
            
            echo "repo_owner=$repo_owner" >> $GITHUB_OUTPUT
            echo "repo_name=$repo_name" >> $GITHUB_OUTPUT
            echo "pr_number=$pr_number" >> $GITHUB_OUTPUT
            echo "full_repo_name=$full_repo_name" >> $GITHUB_OUTPUT
            echo "pr_link=$pr_link" >> $GITHUB_OUTPUT
            echo "repo_url=https://github.com/$repo_owner/$repo_name.git" >> $GITHUB_OUTPUT
          else
            echo "ERROR: Could not parse repository info from PR link: $pr_link"
            exit 1
          fi

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install tensorflow==2.12.0 pandas joblib scipy numpy urllib3
          sudo apt-get update
          sudo apt-get install -y cloc git

      - name: Download scripts
        run: |
          echo "Downloading scripts from aqa-test-tools repository..."
          curl -L -o ExtractTraditionalFeatures.sh "https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/ExtractTraditionalFeatures.sh"
          curl -L -o save_trained_model.py "https://raw.githubusercontent.com/anirudhsengar/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/save_trained_model.py"
          curl -L -o predict.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/predict.py"
          curl -L -o REPD_Impl.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/REPD_Impl.py"
          curl -L -o autoencoder.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/autoencoder.py"
          curl -L -o stat_util.py "https://raw.githubusercontent.com/adoptium/aqa-test-tools/refs/heads/master/BugPredict/GlitchWitcher/stat_util.py"
          chmod +x ExtractTraditionalFeatures.sh

      - name: Check Dataset Availability
        id: check-dataset
        run: |
          dataset_url="https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/${{ steps.parse-command.outputs.full_repo_name }}/${{ steps.parse-command.outputs.full_repo_name }}.csv"
          model_metadata_url="https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/${{ steps.parse-command.outputs.full_repo_name }}/trained_model/metadata.json"

          echo "Checking dataset availability..."
          echo "Dataset URL: $dataset_url"
          echo "Model metadata URL: $model_metadata_url"

          dataset_exists="false"
          model_exists="false"

          # Check if dataset CSV exists
          if curl -s --head --fail "$dataset_url" > /dev/null 2>&1; then
            echo "Dataset CSV found for ${{ steps.parse-command.outputs.full_repo_name }}"
            dataset_exists="true"
          else
            echo "Dataset CSV not found for ${{ steps.parse-command.outputs.full_repo_name }}"
          fi

          # Check if trained model exists
          if curl -s --head --fail "$model_metadata_url" > /dev/null 2>&1; then
            echo "Trained model found for ${{ steps.parse-command.outputs.full_repo_name }}"
            model_exists="true"
          else
            echo "Trained model not found for ${{ steps.parse-command.outputs.full_repo_name }}"
          fi

          echo "dataset_exists=$dataset_exists" >> $GITHUB_OUTPUT
          echo "model_exists=$model_exists" >> $GITHUB_OUTPUT
          echo "dataset_url=$dataset_url" >> $GITHUB_OUTPUT

      - name: Generate Dataset if Missing
        if: steps.check-dataset.outputs.dataset_exists == 'false'
        run: |
          echo "Generating dataset for ${{ steps.parse-command.outputs.full_repo_name }}..."
          ./ExtractTraditionalFeatures.sh "${{ steps.parse-command.outputs.repo_url }}"

          # Find the generated CSV file
          csv_file=$(find . -name "*.csv" -path "./metrics_output_*" | head -1)
          if [ ! -f "$csv_file" ]; then
            echo "ERROR: Failed to generate CSV file"
            exit 1
          fi

          echo "Generated CSV file: $csv_file"
          echo "csv_file_path=$csv_file" >> $GITHUB_ENV

      - name: Train Model if Missing
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        run: |
          echo "Training model for ${{ steps.parse-command.outputs.full_repo_name }}..."

          # Determine dataset path
          if [ "${{ steps.check-dataset.outputs.dataset_exists }}" == "true" ]; then
            dataset_path="${{ steps.check-dataset.outputs.dataset_url }}"
          else
            dataset_path="$csv_file_path"
          fi

          echo "Using dataset: $dataset_path"
          python3 save_trained_model.py "$dataset_path"

          if [ ! -d "trained_model" ]; then
            echo "ERROR: Failed to generate trained model"
            exit 1
          fi

          echo "Model training completed successfully"

      - name: Create PR to aqa-triage-data
        if: steps.check-dataset.outputs.dataset_exists == 'false' || steps.check-dataset.outputs.model_exists == 'false'
        run: |
          echo "Creating PR to anirudhsengar/aqa-triage-data..."

          # Clone the aqa-triage-data repository
          git clone https://github.com/anirudhsengar/aqa-triage-data.git
          cd aqa-triage-data

          # Configure git
          git config user.name "GlitchWitcher Bot"
          git config user.email "glitchwicher-bot@anirudhsengar.net"

          # Create target directory
          target_dir="GlitchWitcher/Traditional Dataset/${{ steps.parse-command.outputs.full_repo_name }}"
          mkdir -p "$target_dir"

          # Copy files to target directory
          if [ "${{ steps.check-dataset.outputs.dataset_exists }}" == "false" ]; then
            echo "Copying CSV file..."
            cp "../$csv_file_path" "$target_dir/${{ steps.parse-command.outputs.full_repo_name }}.csv"
          fi

          if [ "${{ steps.check-dataset.outputs.model_exists }}" == "false" ]; then
            echo "Copying trained model..."
            cp -r "../trained_model" "$target_dir/"
          fi

          # Create branch and commit
          branch_name="add-dataset-${{ steps.parse-command.outputs.full_repo_name }}-$(date +%Y%m%d-%H%M%S)"
          git checkout -b "$branch_name"
          git add .
          git commit -m "Add dataset and trained model for ${{ steps.parse-command.outputs.full_repo_name }}"

          # Push branch (assuming GITHUB_TOKEN has push access or using a bot token)
          git push origin "$branch_name"

          # Create PR using GitHub CLI or API
          cat > pr_body.md << EOF
          # GlitchWitcher Dataset Addition

          This PR adds the dataset and trained model for repository: **${{ steps.parse-command.outputs.repo_owner }}/${{ steps.parse-command.outputs.repo_name }}**

          ## Contents:
          - Dataset CSV file: \`${{ steps.parse-command.outputs.full_repo_name }}.csv\`
          - Trained model directory: \`trained_model/\`

          ## Triggered by:
          - Comment in: ${{ github.event.issue.html_url }}
          - Target PR: ${{ steps.parse-command.outputs.pr_link }}

          This PR was automatically generated by the GlitchWitcher workflow.
          EOF

          # Create PR using GitHub API
          curl -X POST \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/anirudhsengar/aqa-triage-data/pulls \
            -d '{
              "title": "Add GlitchWitcher dataset for ${{ steps.parse-command.outputs.full_repo_name }}",
              "head": "'$branch_name'",
              "base": "main",
              "body": "'"$(cat pr_body.md | sed 's/"/\\"/g' | tr '\n' ' ')"'"
            }'

      - name: Run Analysis on Target PR
        id: analysis
        run: |
          echo "Running GlitchWitcher analysis on ${{ steps.parse-command.outputs.pr_link }}..."

          # Get PR details using GitHub API
          pr_api_url="https://api.github.com/repos/${{ steps.parse-command.outputs.repo_owner }}/${{ steps.parse-command.outputs.repo_name }}/pulls/${{ steps.parse-command.outputs.pr_number }}"

          pr_info=$(curl -s -H "Accept: application/vnd.github.v3+json" "$pr_api_url")
          base_sha=$(echo "$pr_info" | grep -o '"sha": *"[^"]*"' | head -1 | sed 's/"sha": *"\([^"]*\)"/\1/')
          head_sha=$(echo "$pr_info" | grep -o '"sha": *"[^"]*"' | tail -1 | sed 's/"sha": *"\([^"]*\)"/\1/')

          echo "Base SHA: $base_sha"
          echo "Head SHA: $head_sha"

          # Clone the target repository
          git clone "${{ steps.parse-command.outputs.repo_url }}" target_repo
          cd target_repo

          # Get changed files
          git fetch origin
          changed_files=$(git diff --name-only "$base_sha..$head_sha" | grep -E "\.(c|cpp|cxx|cc|h|hpp|hxx)$" || echo "")

          if [ -z "$changed_files" ]; then
            echo "No C/C++ files changed in this PR"
            echo "analysis_result=No C/C++ files found in the PR changes." >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Changed files: $changed_files"

          # Extract features for base commit
          git checkout "$base_sha"
          mkdir -p ../metrics_base
          echo "File,loc,v(g),ev(g),iv(g),n,v,l,d,i,e,b,t,lOComment,lOBlank,LOCodeAndComment,uniq_Op,Uniq_Opnd,total_Op,total_Opnd,branchCount" > ../metrics_base/summary_metrics.csv

          for file in $changed_files; do
            if [ -f "$file" ]; then
              echo "Processing $file (base)..."
              ../extract_traditional_features.sh "$file"
              generated_dir=$(ls -td ../metrics_output_* 2>/dev/null | head -n 1)
              if [ -d "$generated_dir" ] && [ -f "$generated_dir/summary_metrics.csv" ]; then
                tail -n +2 "$generated_dir/summary_metrics.csv" >> ../metrics_base/summary_metrics.csv
                rm -rf "$generated_dir"
              fi
            fi
          done

          # Extract features for head commit
          git checkout "$head_sha"
          mkdir -p ../metrics_head
          echo "File,loc,v(g),ev(g),iv(g),n,v,l,d,i,e,b,t,lOComment,lOBlank,LOCodeAndComment,uniq_Op,Uniq_Opnd,total_Op,total_Opnd,branchCount" > ../metrics_head/summary_metrics.csv

          for file in $changed_files; do
            if [ -f "$file" ]; then
              echo "Processing $file (head)..."
              ../extract_traditional_features.sh "$file"
              generated_dir=$(ls -td ../metrics_output_* 2>/dev/null | head -n 1)
              if [ -d "$generated_dir" ] && [ -f "$generated_dir/summary_metrics.csv" ]; then
                tail -n +2 "$generated_dir/summary_metrics.csv" >> ../metrics_head/summary_metrics.csv
                rm -rf "$generated_dir"
              fi
            fi
          done

          cd ..

          # Run predictions if we have a model
          if [ "${{ steps.check-dataset.outputs.model_exists }}" == "true" ] || [ -d "trained_model" ]; then
            echo "Running predictions..."
            
            # Download existing model if we didn't train one
            if [ "${{ steps.check-dataset.outputs.model_exists }}" == "true" ] && [ ! -d "trained_model" ]; then
              mkdir -p trained_model
              base_url="https://raw.githubusercontent.com/anirudhsengar/aqa-triage-data/refs/heads/main/GlitchWitcher/Traditional%20Dataset/${{ steps.parse-command.outputs.full_repo_name }}/trained_model"
              curl -L -o trained_model/metadata.json "$base_url/metadata.json"
              curl -L -o trained_model/model.pkl "$base_url/model.pkl" || echo "model.pkl not found"
              curl -L -o trained_model/preprocessor.pkl "$base_url/preprocessor.pkl" || echo "preprocessor.pkl not found"
            fi
            
            # Create comparison script
            cat > compare_predictions.py << 'EOF'

      - name: Comment Results
        uses: actions/github-script@v6
        env:
          ANALYSIS_RESULT: "${{ steps.analysis.outputs.analysis_result }}"
          PR_LINK: "${{ steps.parse-command.outputs.pr_link }}"
          REPO_NAME: "${{ steps.parse-command.outputs.full_repo_name }}"
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const commentBody = `## 🔮 GlitchWitcher Analysis Results

            **Target PR:** ${process.env.PR_LINK}
            **Repository:** ${process.env.REPO_NAME}

            ${process.env.ANALYSIS_RESULT}

            ---

            ### 📋 Interpretation Note:
            > The values shown are Probability Densities (PDFs), not probabilities. They represent the model's assessment of how likely a file's characteristics are to be 'defective' vs. 'non-defective'. A higher value indicates a better fit for that category. Very small values are expected and normal.

            *Analysis performed by GlitchWitcher Bot*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });
